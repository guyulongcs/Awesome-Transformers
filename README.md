## Awesome Transformer Papers.

## Efficient-Transformers
* [2017 (NIPS) Attention Is All You Need](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2017%20%28NIPS%29%20Attention%20Is%20All%20You%20Need.pdf) <br />
* [2019 (ACL) [Transformer-XL] Transformer-XL - Attentive Language Models Beyond a Fixed-Length Context](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ACL%29%20%5BTransformer-XL%5D%20Transformer-XL%20-%20Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.pdf) <br />
* [2019 (Arxiv) Axial Attention in Multidimensional Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28Arxiv%29%20Axial%20Attention%20in%20Multidimensional%20Transformers.pdf) <br />
* [2019 (Arxiv) Generating Long Sequences with Sparse Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28Arxiv%29%20Generating%20Long%20Sequences%20with%20Sparse%20Transformers.pdf) <br />
* [2019 (EMNLP) Adaptively Sparse Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28EMNLP%29%20Adaptively%20Sparse%20Transformers.pdf) <br />
* [2019 (ICLR) Universal Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ICLR%29%20Universal%20Transformers.pdf) <br />
* [2019 (ICML) Parameter-Efficient Transfer Learning for NLP](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ICML%29%20Parameter-Efficient%20Transfer%20Learning%20for%20NLP.pdf) <br />
* [2019 (ICML) Sparse Sinkhorn Attention](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ICML%29%20Sparse%20Sinkhorn%20Attention.pdf) <br />
* [2019 (ICML) The Evolved Transformer](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ICML%29%20The%20Evolved%20Transformer.pdf) <br />
* [2019 (ICML) [SetTransformer] Set Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28ICML%29%20%5BSetTransformer%5D%20Set%20Transformer%20-%20A%20Framework%20for%20Attention-based%20Permutation-Invariant%20Neural%20Networks.pdf) <br />
* [2019 (NAACL) Star-Transformer](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28NAACL%29%20Star-Transformer.pdf) <br />
* [2019 (NIPS) Stand-Alone Self-Attention in Vision Models](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28NIPS%29%20Stand-Alone%20Self-Attention%20in%20Vision%20Models.pdf) <br />
* [2019 (NIPS) [NAT] NAT - Neural Architecture Transformer for Accurate and Compact Architectures](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2019%20%28NIPS%29%20%5BNAT%5D%20NAT%20-%20Neural%20Architecture%20Transformer%20for%20Accurate%20and%20Compact%20Architectures.pdf) <br />
* [2020 (Arxiv) Efficient Content-Based Sparse Attention with Routing Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20Efficient%20Content-Based%20Sparse%20Attention%20with%20Routing%20Transformers.pdf) <br />
* [2020 (Arxiv) Efficient Transformers - A Survey](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20Efficient%20Transformers%20-%20A%20Survey.pdf) <br />
* [2020 (Arxiv) HyperGrid - Efficient Multi-Task Transformers with Grid-wise Decomposable Hyper Projections](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20HyperGrid%20-%20Efficient%20Multi-Task%20Transformers%20with%20Grid-wise%20Decomposable%20Hyper%20Projections.pdf) <br />
* [2020 (Arxiv) Linformer - Self-Attention with Linear Complexity](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20Linformer%20-%20Self-Attention%20with%20Linear%20Complexity.pdf) <br />
* [2020 (Arxiv) Longformer - The Long-Document Transformer](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20Longformer%20-%20The%20Long-Document%20Transformer.pdf) <br />
* [2020 (Arxiv) Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20Masked%20Language%20Modeling%20for%20Proteins%20via%20Linearly%20Scalable%20Long-Context%20Transformers.pdf) <br />
* [2020 (Arxiv) [AdpaterHub] AdapterHub - A Framework for Adapting Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20%5BAdpaterHub%5D%20AdapterHub%20-%20A%20Framework%20for%20Adapting%20Transformers.pdf) <br />
* [2020 (Arxiv) [RealFormer] RealFormer -Transformer Likes Residual Attention](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20%5BRealFormer%5D%20RealFormer%20-Transformer%20Likes%20Residual%20Attention.pdf) <br />
* [2020 (Arxiv) [Synthesizer] Synthesizer - Rethinking Self-Attention in Transformer Models](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28Arxiv%29%20%5BSynthesizer%5D%20Synthesizer%20-%20Rethinking%20Self-Attention%20in%20Transformer%20Models.pdf) <br />
* [2020 (EMNLP Findings) [BlockBERT] Blockwise Self-Attention for Long Document Understanding](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28EMNLP%20Findings%29%20%5BBlockBERT%5D%20Blockwise%20Self-Attention%20for%20Long%20Document%20Understanding.pdf) <br />
* [2020 (EMNLP) [ETC] ETC - Encoding Long and Structured Inputs in Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28EMNLP%29%20%5BETC%5D%20ETC%20-%20Encoding%20Long%20and%20Structured%20Inputs%20in%20Transformers.pdf) <br />
* [2020 (ICML) Transformers are RNNs - Fast Autoregressive Transformers with Linear Attention](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28ICML%29%20Transformers%20are%20RNNs%20-%20Fast%20Autoregressive%20Transformers%20with%20Linear%20Attention.pdf) <br />
* [2020 (NIPS) [BigBird] Big Bird - Transformers for Longer Sequences](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2020%20%28NIPS%29%20%5BBigBird%5D%20Big%20Bird%20-%20Transformers%20for%20Longer%20Sequences.pdf) <br />
* [2021 (AAAI) [Informer] Informer - Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2021%20%28AAAI%29%20%5BInformer%5D%20Informer%20-%20Beyond%20Efficient%20Transformer%20for%20Long%20Sequence%20Time-Series%20Forecasting.pdf) <br />
* [2021 (ICLR) Long Range Arena - A Benchmark for Efficient Transformers](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2021%20%28ICLR%29%20Long%20Range%20Arena%20-%20A%20Benchmark%20for%20Efficient%20Transformers.pdf) <br />
* [2021 (ICLR) Training with Quantization Noise for Extreme Model Compression](https://github.com/guyulongcs/Deep-Learning-for-Search-Recommendation-Advertisements/blob/master/Efficient-Transformers/2021%20%28ICLR%29%20Training%20with%20Quantization%20Noise%20for%20Extreme%20Model%20Compression.pdf) <br />
